import { createClient } from '@supabase/supabase-js';
import { AdminElasticsearchService } from './elasticsearchService';
import cloudinaryService from './cloudinaryService';
import logger from '../config/logger';

interface QueueJob {
  id: number;
  table_name: string;
  operation: string;
  record_id: string;
  change_data: any;
  status: string;
  created_at: string;
  processed_at?: string;
  error_message?: string;
  retry_count: number;
}

interface QueueStats {
  total: number;
  pending: number;
  processing: number;
  completed: number;
  failed: number;
  stuck: number;
  avgProcessingTime: number;
  lastProcessedAt?: string;
}

export class QueueProcessorService {
  private supabase: any;
  private elasticsearchService: AdminElasticsearchService;
  private isProcessing: boolean = false;
  private processingInterval: NodeJS.Timeout | null = null;
  private healthCheckInterval: NodeJS.Timeout | null = null;
  private stuckJobTimeout: number = 30 * 1000; // 30 saniye (√ßok agresif)
  private maxRetries: number = 3;
  private batchSize: number = 5;
  private processingTimeout: number = 30 * 1000; // 30 saniye
  private stats: QueueStats = {
    total: 0,
    pending: 0,
    processing: 0,
    completed: 0,
    failed: 0,
    stuck: 0,
    avgProcessingTime: 0
  };

  constructor() {
    this.supabase = createClient(
      process.env.SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
    this.elasticsearchService = new AdminElasticsearchService();
  }

  /**
   * Queue processing'i ba≈ülat
   */
  async startProcessing(intervalMs: number = 5000): Promise<void> {
    if (this.isProcessing) {
      logger.warn('‚ö†Ô∏è Queue processing already running');
      return;
    }

    logger.info('üöÄ Starting enhanced Elasticsearch queue processor...');
    this.isProcessing = true;

    // ƒ∞lk i≈ülemi hemen yap
    await this.processQueue();

    // Periyodik i≈ülem
    this.processingInterval = setInterval(async () => {
      await this.processQueue();
    }, intervalMs);

    // Health check interval
    this.healthCheckInterval = setInterval(async () => {
      await this.healthCheck();
    }, 15000); // 15 saniye (daha sƒ±k kontrol)

    logger.info('‚úÖ Enhanced queue processor started successfully');
  }

  /**
   * Queue processing'i durdur
   */
  async stopProcessing(): Promise<void> {
    if (!this.isProcessing) {
      return;
    }

    logger.info('üõë Stopping enhanced Elasticsearch queue processor...');
    this.isProcessing = false;

    if (this.processingInterval) {
      clearInterval(this.processingInterval);
      this.processingInterval = null;
    }

    if (this.healthCheckInterval) {
      clearInterval(this.healthCheckInterval);
      this.healthCheckInterval = null;
    }

    logger.info('‚úÖ Enhanced queue processor stopped');
  }

  /**
   * Health check - stuck job'larƒ± tespit et ve d√ºzelt (Enhanced with detailed analysis)
   */
  private async healthCheck(): Promise<void> {
    try {
      const stuckJobs = await this.detectStuckJobs();
      
      if (stuckJobs.length > 0) {
        logger.warn(`‚ö†Ô∏è Found ${stuckJobs.length} stuck jobs, auto-fixing...`);
        
        // Stuck job'larƒ± analiz et
        const stuckJobAnalysis = stuckJobs.map(job => {
          const stuckDuration = Math.round((Date.now() - new Date(job.created_at).getTime()) / 1000 / 60);
          const dataSize = JSON.stringify(job.change_data).length;
          return {
            id: job.id,
            table: job.table_name,
            operation: job.operation,
            record_id: job.record_id,
            stuckDuration,
            retryCount: job.retry_count,
            dataSize,
            hasLargeData: dataSize > 1000000,
            hasBase64Data: JSON.stringify(job.change_data).includes('base64')
          };
        });
        
        logger.warn(`üîç Stuck jobs analysis:`, stuckJobAnalysis);
        
        let fixedCount = 0;
        let failedCount = 0;
        
        for (const job of stuckJobs) {
          try {
            // Base64 data i√ßeren job'larƒ± direkt failed yap
            const changeDataStr = JSON.stringify(job.change_data);
            if (changeDataStr.includes('base64') && changeDataStr.length > 100000) {
              logger.warn(`üö´ Base64 job detected - marking as permanently failed: ${job.id}`);
              await this.updateJobStatus(
                job.id, 
                'failed', 
                'Job contains large base64 data - permanently failed to prevent infinite loops'
              );
              failedCount++;
            } else {
              await this.resetStuckJob(job);
              fixedCount++;
            }
          } catch (error) {
            logger.error(`‚ùå Failed to reset stuck job ${job.id}:`, error);
            failedCount++;
          }
        }
        
        logger.info(`‚úÖ Health check completed: ${fixedCount} jobs fixed, ${failedCount} failed`, {
          totalStuckJobs: stuckJobs.length,
          fixedCount,
          failedCount,
          stuckJobAnalysis
        });
        
        // Eƒüer √ßok fazla stuck job varsa uyarƒ± ver
        if (stuckJobs.length > 5) {
          logger.error(`üö® CRITICAL: ${stuckJobs.length} stuck jobs detected! Queue processor may have issues.`, {
            stuckJobCount: stuckJobs.length,
            stuckJobAnalysis
          });
        }
      } else {
        logger.debug('‚úÖ Health check: No stuck jobs found');
      }

      // Stats g√ºncelle
      await this.updateStats();
      
    } catch (error) {
      logger.error('‚ùå Error in health check:', error);
    }
  }

  /**
   * Stuck job'larƒ± tespit et (Enhanced)
   */
  private async detectStuckJobs(): Promise<any[]> {
    try {
      const cutoffTime = new Date(Date.now() - this.stuckJobTimeout);
      
      // 1. Zaman bazlƒ± stuck job'lar
      const { data: timeBasedStuckJobs, error: timeError } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .eq('status', 'processing')
        .lt('created_at', cutoffTime.toISOString());

      if (timeError) {
        logger.error('‚ùå Error detecting time-based stuck jobs:', timeError);
      }

      // 2. √áok uzun s√ºredir processing'de olan job'lar (10 dakika)
      const longStuckCutoff = new Date(Date.now() - 10 * 60 * 1000);
      const { data: longStuckJobs, error: longError } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .eq('status', 'processing')
        .lt('created_at', longStuckCutoff.toISOString());

      if (longError) {
        logger.error('‚ùå Error detecting long stuck jobs:', longError);
      }

      // 3. √áok fazla retry yapmƒ±≈ü ama hala processing'de olan job'lar
      const { data: highRetryJobs, error: retryError } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .eq('status', 'processing')
        .gte('retry_count', 2);

      if (retryError) {
        logger.error('‚ùå Error detecting high retry jobs:', retryError);
      }

      // T√ºm stuck job'larƒ± birle≈ütir ve unique yap
      const allStuckJobs = [
        ...(timeBasedStuckJobs || []),
        ...(longStuckJobs || []),
        ...(highRetryJobs || [])
      ];

      const uniqueStuckJobs = allStuckJobs.filter((job, index, self) => 
        index === self.findIndex(j => j.id === job.id)
      );

      if (uniqueStuckJobs.length > 0) {
        logger.warn(`‚ö†Ô∏è Detected ${uniqueStuckJobs.length} stuck jobs:`, 
          uniqueStuckJobs.map(job => ({
            id: job.id,
            table: job.table_name,
            operation: job.operation,
            retryCount: job.retry_count,
            stuckFor: Math.round((Date.now() - new Date(job.created_at).getTime()) / 1000 / 60) + ' minutes'
          }))
        );
      }

      return uniqueStuckJobs;
    } catch (error) {
      logger.error('‚ùå Error detecting stuck jobs:', error);
      return [];
    }
  }

  /**
   * Stuck job'ƒ± reset et (Enhanced with detailed analysis)
   */
  private async resetStuckJob(job: any): Promise<void> {
    try {
      const stuckDuration = Math.round((Date.now() - new Date(job.created_at).getTime()) / 1000 / 60);
      const retryCount = job.retry_count || 0;
      const dataSize = JSON.stringify(job.change_data).length;
      
      // Job analizi
      const jobAnalysis = {
        id: job.id,
        operation: job.operation,
        table: job.table_name,
        record_id: job.record_id,
        stuckDuration,
        retryCount,
        dataSize,
        hasLargeData: dataSize > 1000000, // 1MB
        hasBase64Data: JSON.stringify(job.change_data).includes('base64'),
        created_at: job.created_at
      };
      
      logger.warn(`üîç Stuck job analysis for job ${job.id}:`, jobAnalysis);
      
      // Eƒüer √ßok fazla retry yapmƒ±≈üsa failed olarak i≈üaretle
      if (retryCount >= this.maxRetries) {
        const failureReason = jobAnalysis.hasLargeData 
          ? `Job stuck for ${stuckDuration} minutes with large data (${dataSize} bytes) and exceeded max retries (${this.maxRetries})`
          : `Job stuck for ${stuckDuration} minutes and exceeded max retries (${this.maxRetries})`;
          
        await this.updateJobStatus(job.id, 'failed', failureReason);
        logger.warn(`‚ùå Marked stuck job ${job.id} as failed (max retries exceeded)`, jobAnalysis);
      } else {
        // Normal reset
        let resetReason = `Reset from stuck state (stuck for ${stuckDuration} minutes, retry ${retryCount + 1}/${this.maxRetries})`;
        if (jobAnalysis.hasLargeData) {
          resetReason += `. Large data detected: ${dataSize} bytes`;
        }
        
        await this.updateJobStatus(job.id, 'pending', resetReason);
        logger.info(`üîÑ Reset stuck job ${job.id} to pending (stuck for ${stuckDuration} minutes)`, jobAnalysis);
      }
    } catch (error) {
      logger.error(`‚ùå Error resetting stuck job ${job.id}:`, error);
    }
  }

  /**
   * Queue'daki i≈üleri i≈üle (Enhanced)
   */
  private async processQueue(): Promise<void> {
    try {
      // Pending job'larƒ± al (batch size kadar)
      const { data: jobs, error } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .eq('status', 'pending')
        .order('created_at', { ascending: true })
        .limit(this.batchSize);

      if (error) {
        logger.error('‚ùå Error fetching queue jobs:', error);
        return;
      }

      if (!jobs || jobs.length === 0) {
        return; // ƒ∞≈ü yok
      }

      logger.info(`üîÑ Processing ${jobs.length} queue jobs (batch ${this.batchSize})...`);
      logger.info(`üìã Jobs to process:`, jobs.map(job => ({ id: job.id, operation: job.operation, table: job.table_name, record_id: job.record_id })));

      // Batch processing
      const promises = jobs.map((job: any) => this.processJobWithTimeout(job));
      const results = await Promise.allSettled(promises);

      // Sonu√ßlarƒ± analiz et
      const successful = results.filter(r => r.status === 'fulfilled').length;
      const failed = results.filter(r => r.status === 'rejected').length;

      logger.info(`‚úÖ Batch completed: ${successful} successful, ${failed} failed`);

    } catch (error) {
      // Supabase baƒülantƒ± hatasƒ± durumunda sessizce devam et
      if (error instanceof Error && error.message.includes('fetch failed')) {
        logger.debug('üîá Queue processing skipped due to connection issue');
        return;
      }
      logger.error('‚ùå Error in processQueue:', error);
    }
  }

  /**
   * Job'ƒ± timeout ile i≈üle (Enhanced with detailed logging)
   */
  private async processJobWithTimeout(job: QueueJob): Promise<void> {
    const startTime = Date.now();
    const jobDetails = {
      id: job.id,
      operation: job.operation,
      table: job.table_name,
      record_id: job.record_id,
      retry_count: job.retry_count,
      data_size: JSON.stringify(job.change_data).length
    };

    try {
      logger.info(`üîÑ Starting job ${job.id}: ${job.operation} on ${job.table_name}:${job.record_id}`, {
        jobDetails,
        timeout: this.processingTimeout,
        maxRetries: this.maxRetries
      });
      
      // Data size kontrol√º
      if (jobDetails.data_size > 1000000) { // 1MB
        logger.warn(`‚ö†Ô∏è Large job data detected: ${jobDetails.data_size} bytes for job ${job.id}`);
      }
      
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error(`Job processing timeout after ${this.processingTimeout}ms`)), this.processingTimeout);
      });

      const jobPromise = this.processJob(job);

      await Promise.race([jobPromise, timeoutPromise]);
      
      const processingTime = Date.now() - startTime;
      logger.info(`‚úÖ Job ${job.id} completed successfully in ${processingTime}ms`, {
        jobDetails,
        processingTime,
        dataSize: jobDetails.data_size
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      logger.error(`‚ùå Job ${job.id} failed or timed out after ${processingTime}ms:`, {
        jobDetails,
        error: errorMessage,
        processingTime,
        dataSize: jobDetails.data_size,
        stack: error instanceof Error ? error.stack : undefined
      });
      
      // Detaylƒ± error message olu≈ütur
      const detailedErrorMessage = `Job failed after ${processingTime}ms: ${errorMessage}. Data size: ${jobDetails.data_size} bytes. Retry: ${job.retry_count}/${this.maxRetries}`;
      
      // Job'ƒ± failed olarak i≈üaretle
      await this.updateJobStatus(job.id, 'failed', detailedErrorMessage);
    }
  }

  /**
   * Tek bir job'ƒ± i≈üle (Enhanced)
   */
  private async processJob(job: QueueJob): Promise<void> {
    const startTime = Date.now();
    
    try {
      // Job'ƒ± processing olarak i≈üaretle
      await this.updateJobStatus(job.id, 'processing');

      const { table_name, operation, record_id, change_data } = job;

      logger.info(`üîÑ Processing job ${job.id}: ${operation} on ${table_name}:${record_id}`);

      // Retry count kontrol√º
      if (job.retry_count >= this.maxRetries) {
        throw new Error(`Max retries (${this.maxRetries}) exceeded`);
      }

      switch (table_name) {
        case 'listings':
          logger.info(`üìù Processing listing job: ${operation} on ${record_id}`);
          await this.processListingJob(operation, record_id, change_data);
          break;
        case 'profiles':
          logger.info(`üìù Processing profile job: ${operation} on ${record_id}`);
          await this.processProfileJob(operation, record_id, change_data);
          break;
        case 'categories':
          logger.info(`üìù Processing category job: ${operation} on ${record_id}`);
          await this.processCategoryJob(operation, record_id, change_data);
          break;
        case 'category_ai_suggestions':
          logger.info(`üìù Processing AI suggestion job: ${operation} on ${record_id}`);
          await this.processAiSuggestionJob(operation, record_id, change_data);
          break;
        case 'inventory_items':
          logger.info(`üìù Processing inventory job: ${operation} on ${record_id}`);
          await this.processInventoryJob(operation, record_id, change_data);
          break;
        default:
          logger.warn(`‚ö†Ô∏è Unknown table: ${table_name}`);
          await this.updateJobStatus(job.id, 'failed', `Unknown table: ${table_name}`);
          return;
      }

      // Job'ƒ± completed olarak i≈üaretle
      await this.updateJobStatus(job.id, 'completed');
      
      const processingTime = Date.now() - startTime;
      logger.info(`‚úÖ Job ${job.id} completed successfully in ${processingTime}ms`);

    } catch (error) {
      const processingTime = Date.now() - startTime;
      logger.error(`‚ùå Error processing job ${job.id} (${processingTime}ms):`, error);
      
      // Retry logic
      if (job.retry_count < this.maxRetries) {
        await this.updateJobStatus(job.id, 'pending', `Retry ${job.retry_count + 1}/${this.maxRetries}: ${error instanceof Error ? error.message : String(error)}`);
        logger.info(`üîÑ Job ${job.id} queued for retry (${job.retry_count + 1}/${this.maxRetries})`);
      } else {
        await this.updateJobStatus(job.id, 'failed', `Max retries exceeded: ${error instanceof Error ? error.message : String(error)}`);
      }
    }
  }

  /**
   * Listing job'ƒ±nƒ± i≈üle (Enhanced)
   */
  private async processListingJob(operation: string, recordId: string, changeData: any): Promise<void> {
    try {
      switch (operation) {
        case 'INSERT':
          // Yeni listing eklendi
          if (changeData.status === 'active') {
            await this.elasticsearchService.indexDocument(recordId, this.transformListingForElasticsearch(changeData));
            
            // Kategori sayƒ±larƒ± cache'ini temizle
            await this.elasticsearchService.invalidateCategoryCountsCache();
            logger.info(`‚úÖ Category counts cache invalidated after new listing: ${recordId}`);
          }
          break;

        case 'UPDATE':
          // Listing g√ºncellendi
          const newData = changeData.new;
          const oldData = changeData.old;

          if (newData.status === 'active' && oldData.status !== 'active') {
            // ƒ∞lan onaylandƒ± - Elasticsearch'e ekle
            await this.elasticsearchService.indexDocument(recordId, this.transformListingForElasticsearch(newData));
            
            // Kategori sayƒ±larƒ± cache'ini temizle
            await this.elasticsearchService.invalidateCategoryCountsCache();
            logger.info(`‚úÖ Category counts cache invalidated after listing approval: ${recordId}`);
          } else if (newData.status === 'active' && oldData.status === 'active') {
            // Aktif ilan g√ºncellendi - Elasticsearch'i g√ºncelle
            await this.elasticsearchService.updateDocument(recordId, this.transformListingForElasticsearch(newData));
            
            // Kategori deƒüi≈ümi≈üse cache'i temizle (category_id veya category name)
            if (newData.category_id !== oldData.category_id || newData.category !== oldData.category) {
              await this.elasticsearchService.invalidateCategoryCountsCache();
              logger.info(`‚úÖ Category counts cache invalidated after category change: ${recordId} (${oldData.category} ‚Üí ${newData.category})`);
            }
          } else if (newData.status !== 'active' && oldData.status === 'active') {
            // ƒ∞lan deaktif edildi (inactive, rejected, deleted, pending_approval) - Elasticsearch'ten sil
            await this.elasticsearchService.deleteDocument(recordId);
            
            // Kategori sayƒ±larƒ± cache'ini temizle
            await this.elasticsearchService.invalidateCategoryCountsCache();
            logger.info(`‚úÖ Category counts cache invalidated after listing deactivation: ${recordId}`);
          }
          break;

        case 'DELETE':
          // Listing silindi
          logger.info(`üóëÔ∏è Processing listing DELETE job: ${recordId}`);
          
          // Akƒ±llƒ± kontrol: Record'ƒ±n ger√ßekten var olup olmadƒ±ƒüƒ±nƒ± kontrol et
          const recordExists = await this.checkRecordExists('listings', recordId);
          
          if (!recordExists) {
            logger.warn(`‚ö†Ô∏è Record ${recordId} already deleted or doesn't exist - skipping DELETE job`, {
              recordId,
              operation: 'DELETE',
              table: 'listings'
            });
            // Job'ƒ± completed olarak i≈üaretle √ß√ºnk√º zaten silinmi≈ü
            return;
          }
          
          await this.elasticsearchService.deleteDocument(recordId);
          
          // Kategori sayƒ±larƒ± cache'ini temizle
          await this.elasticsearchService.invalidateCategoryCountsCache();
          logger.info(`‚úÖ Category counts cache invalidated after listing deletion: ${recordId}`);
          break;

        default:
          throw new Error(`Unknown operation: ${operation}`);
      }
    } catch (error) {
      logger.error(`‚ùå Error processing listing job:`, error);
      throw error;
    }
  }

  /**
   * Profile job'ƒ±nƒ± i≈üle (Enhanced)
   */
  private async processProfileJob(operation: string, recordId: string, changeData: any): Promise<void> {
    try {
      // Profile deƒüi≈üiklikleri i√ßin ≈üimdilik sadece log
      logger.info(`üìù Profile ${operation}: ${recordId}`);
      // TODO: Profile deƒüi≈üikliklerini ilgili listing'lere yansƒ±t
    } catch (error) {
      logger.error(`‚ùå Error processing profile job:`, error);
      throw error;
    }
  }

  /**
   * Category job'ƒ±nƒ± i≈üle (Enhanced)
   */
  private async processCategoryJob(operation: string, recordId: string, changeData: any): Promise<void> {
    try {
      // Category deƒüi≈üiklikleri i√ßin ≈üimdilik sadece log
      logger.info(`üìù Category ${operation}: ${recordId}`);
      // TODO: Category deƒüi≈üikliklerini ilgili listing'lere yansƒ±t
    } catch (error) {
      logger.error(`‚ùå Error processing category job:`, error);
      throw error;
    }
  }

  /**
   * Inventory job'ƒ±nƒ± i≈üle (Enhanced with detailed logging)
   */
  private async processInventoryJob(operation: string, recordId: string, changeData: any): Promise<void> {
    const startTime = Date.now();
    const dataSize = JSON.stringify(changeData).length;
    
    try {
      logger.info(`üì¶ Processing inventory job: ${operation} on ${recordId}`, {
        operation,
        recordId,
        dataSize,
        hasBase64Data: JSON.stringify(changeData).includes('base64'),
        hasImages: changeData?.main_image_url || changeData?.additional_image_urls?.length > 0
      });

      switch (operation) {
        case 'INSERT':
          // Yeni inventory item eklendi
          logger.info(`üì¶ New inventory item added: ${recordId}`, {
            recordId,
            dataSize,
            hasImages: changeData?.main_image_url || changeData?.additional_image_urls?.length > 0
          });
          
          // Base64 data kontrol√º
          if (JSON.stringify(changeData).includes('base64')) {
            logger.warn(`‚ö†Ô∏è Base64 image data detected in inventory INSERT job ${recordId}`, {
              recordId,
              dataSize,
              imageCount: changeData?.additional_image_urls?.length || 0
            });
          }
          
          // TODO: Inventory-specific processing (e.g., recommendation updates, analytics)
          break;

        case 'UPDATE':
          // Inventory item g√ºncellendi
          const newData = changeData.new;
          const oldData = changeData.old;
          
          logger.info(`üì¶ Inventory item updated: ${recordId}`, {
            recordId,
            dataSize,
            hasImageChanges: newData.main_image_url !== oldData.main_image_url || 
                            JSON.stringify(newData.additional_image_urls) !== JSON.stringify(oldData.additional_image_urls)
          });
          
          // Eƒüer imaj deƒüi≈üikliƒüi varsa Cloudinary i≈ülemleri
          if (newData.main_image_url !== oldData.main_image_url || 
              JSON.stringify(newData.additional_image_urls) !== JSON.stringify(oldData.additional_image_urls)) {
            logger.info(`üñºÔ∏è Image change detected for inventory item: ${recordId}`, {
              recordId,
              oldImageCount: oldData.additional_image_urls?.length || 0,
              newImageCount: newData.additional_image_urls?.length || 0
            });
            // TODO: Cloudinary cleanup for old images
          }
          break;

        case 'DELETE':
          // Inventory item silindi
          logger.info(`üóëÔ∏è Processing inventory DELETE job: ${recordId}`, {
            recordId,
            dataSize,
            hasImages: changeData?.main_image_url || changeData?.additional_image_urls?.length > 0,
            userId: changeData?.user_id
          });
          
          // Akƒ±llƒ± kontrol: Record'ƒ±n ger√ßekten var olup olmadƒ±ƒüƒ±nƒ± kontrol et
          const recordExists = await this.checkRecordExists('inventory_items', recordId);
          
          if (!recordExists) {
            logger.warn(`‚ö†Ô∏è Record ${recordId} already deleted or doesn't exist - skipping DELETE job`, {
              recordId,
              operation: 'DELETE',
              table: 'inventory_items'
            });
            // Job'ƒ± completed olarak i≈üaretle √ß√ºnk√º zaten silinmi≈ü
            return;
          }
          
          logger.info(`üóëÔ∏è Inventory item deleted: ${recordId}`, {
            recordId,
            dataSize,
            hasImages: changeData?.main_image_url || changeData?.additional_image_urls?.length > 0,
            userId: changeData?.user_id
          });
          
          // Cloudinary'den imajlarƒ± sil
          if (changeData && changeData.main_image_url || changeData?.additional_image_urls) {
            await this.cleanupInventoryImages(changeData);
          }
          
          // Cloudinary'den klas√∂r√º sil
          if (changeData && changeData.user_id) {
            logger.info(`üóëÔ∏è Attempting to delete folder for user: ${changeData.user_id}, item: ${recordId}`);
            await this.cleanupInventoryFolder(changeData.user_id, recordId);
          } else {
            logger.warn(`‚ö†Ô∏è Cannot delete folder - missing user_id in changeData for job ${recordId}`);
          }
          break;

        default:
          throw new Error(`Unknown operation: ${operation}`);
      }
      
      const processingTime = Date.now() - startTime;
      logger.info(`‚úÖ Inventory job completed: ${operation} on ${recordId} in ${processingTime}ms`, {
        operation,
        recordId,
        processingTime,
        dataSize
      });
      
    } catch (error) {
      const processingTime = Date.now() - startTime;
      logger.error(`‚ùå Error processing inventory job: ${operation} on ${recordId} after ${processingTime}ms`, {
        operation,
        recordId,
        error: error instanceof Error ? error.message : String(error),
        processingTime,
        dataSize,
        stack: error instanceof Error ? error.stack : undefined
      });
      throw error;
    }
  }

  /**
   * Record'ƒ±n veritabanƒ±nda var olup olmadƒ±ƒüƒ±nƒ± kontrol et
   */
  private async checkRecordExists(tableName: string, recordId: string): Promise<boolean> {
    try {
      const { data, error } = await this.supabase
        .from(tableName)
        .select('id')
        .eq('id', recordId)
        .limit(1);

      if (error) {
        logger.error(`‚ùå Error checking record existence: ${tableName}:${recordId}`, error);
        return false;
      }

      const exists = data && data.length > 0;
      logger.debug(`üîç Record existence check: ${tableName}:${recordId} = ${exists}`);
      
      return exists;
    } catch (error) {
      logger.error(`‚ùå Error checking record existence: ${tableName}:${recordId}`, error);
      return false;
    }
  }

  /**
   * Inventory imajlarƒ±nƒ± Cloudinary'den temizle
   */
  private async cleanupInventoryImages(inventoryData: any): Promise<void> {
    try {
      const imageUrls = [
        inventoryData.main_image_url,
        ...(inventoryData.additional_image_urls || [])
      ].filter(Boolean);

      // Cloudinary URL'lerini filtrele
      const cloudinaryUrls = imageUrls.filter(url => url.includes('cloudinary.com'));
      
      if (cloudinaryUrls.length > 0) {
        logger.info(`üóëÔ∏è Cleaning up ${cloudinaryUrls.length} Cloudinary images for deleted inventory item`);
        
        // URL'lerden public ID'leri √ßƒ±kar
        const publicIds = cloudinaryUrls
          .map(url => cloudinaryService.extractPublicId(url))
          .filter(Boolean);

        if (publicIds.length > 0) {
          // Cloudinary'den imajlarƒ± sil
          await cloudinaryService.deleteMultipleImages(publicIds);
          logger.info(`‚úÖ Successfully deleted ${publicIds.length} images from Cloudinary`);
        }
      }
    } catch (error) {
      logger.error('‚ùå Error cleaning up inventory images:', error);
      // Don't throw - image cleanup failure shouldn't fail the job
    }
  }

  /**
   * Inventory klas√∂r√ºn√º Cloudinary'den temizle
   */
  private async cleanupInventoryFolder(userId: string, itemId: string): Promise<void> {
    try {
      const folderPath = `benalsam/inventory/${userId}/${itemId}`;
      logger.info(`üóëÔ∏è Cleaning up inventory folder: ${folderPath}`);
      
      const result = await cloudinaryService.deleteInventoryItemFolder(userId, itemId);
      
      if (result) {
        logger.info(`‚úÖ Successfully deleted inventory folder from Cloudinary: ${folderPath}`);
      } else {
        logger.warn(`‚ö†Ô∏è Failed to delete inventory folder from Cloudinary: ${folderPath}`);
        
        // Alternatif y√∂ntem: Manuel olarak klas√∂r√º silmeyi dene
        logger.info(`üîÑ Trying alternative deletion method for: ${folderPath}`);
        const alternativeResult = await cloudinaryService.deleteFolder(folderPath);
        
        if (alternativeResult) {
          logger.info(`‚úÖ Alternative deletion successful for: ${folderPath}`);
        } else {
          logger.error(`‚ùå Alternative deletion also failed for: ${folderPath}`);
        }
      }
    } catch (error) {
      logger.error('‚ùå Error cleaning up inventory folder:', error);
      logger.error('‚ùå Error details:', {
        userId,
        itemId,
        folderPath: `benalsam/inventory/${userId}/${itemId}`,
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined
      });
      // Don't throw - folder cleanup failure shouldn't fail the job
    }
  }

  /**
   * AI Suggestion job'ƒ±nƒ± i≈üle (Enhanced)
   */
  private async processAiSuggestionJob(operation: string, recordId: string, changeData: any): Promise<void> {
    try {
      switch (operation) {
        case 'INSERT':
          // Yeni AI suggestion eklendi
          if (changeData.is_approved) {
            await this.elasticsearchService.indexDocument(
              `ai_suggestions_${recordId}`, 
              this.transformAiSuggestionForElasticsearch(changeData),
              'ai_suggestions'
            );
          }
          break;

        case 'UPDATE':
          // AI suggestion g√ºncellendi
          const newData = changeData.new;
          const oldData = changeData.old;

          if (newData.is_approved && oldData.is_approved) {
            // Onaylƒ± suggestion g√ºncellendi - Elasticsearch'i g√ºncelle
            await this.elasticsearchService.updateDocument(
              `ai_suggestions_${recordId}`,
              this.transformAiSuggestionForElasticsearch(newData),
              'ai_suggestions'
            );
          } else if (newData.is_approved && !oldData.is_approved) {
            // Suggestion onaylandƒ± - Elasticsearch'e ekle
            await this.elasticsearchService.indexDocument(
              `ai_suggestions_${recordId}`,
              this.transformAiSuggestionForElasticsearch(newData),
              'ai_suggestions'
            );
          } else if (!newData.is_approved && oldData.is_approved) {
            // Suggestion onayƒ± kaldƒ±rƒ±ldƒ± - Elasticsearch'ten sil
            await this.elasticsearchService.deleteDocument(`ai_suggestions_${recordId}`, 'ai_suggestions');
          }
          break;

        case 'DELETE':
          // AI suggestion silindi
          logger.info(`üóëÔ∏è Processing AI suggestion DELETE job: ${recordId}`);
          
          // Akƒ±llƒ± kontrol: Record'ƒ±n ger√ßekten var olup olmadƒ±ƒüƒ±nƒ± kontrol et
          const recordExists = await this.checkRecordExists('category_ai_suggestions', recordId);
          
          if (!recordExists) {
            logger.warn(`‚ö†Ô∏è Record ${recordId} already deleted or doesn't exist - skipping DELETE job`, {
              recordId,
              operation: 'DELETE',
              table: 'category_ai_suggestions'
            });
            // Job'ƒ± completed olarak i≈üaretle √ß√ºnk√º zaten silinmi≈ü
            return;
          }
          
          await this.elasticsearchService.deleteDocument(`ai_suggestions_${recordId}`, 'ai_suggestions');
          break;

        default:
          throw new Error(`Unknown operation: ${operation}`);
      }
    } catch (error) {
      logger.error(`‚ùå Error processing AI suggestion job:`, error);
      throw error;
    }
  }

  /**
   * AI Suggestion'ƒ± Elasticsearch formatƒ±na √ßevir
   */
  private transformAiSuggestionForElasticsearch(suggestion: any): any {
    // Supabase'den gelen suggestion_data'yƒ± ES formatƒ±na √ßevir
    let transformedSuggestionData = { ...suggestion.suggestion_data };
    
    // Eƒüer 'suggestions' varsa 'keywords' olarak kopyala
    if (suggestion.suggestion_data.suggestions) {
      transformedSuggestionData.keywords = suggestion.suggestion_data.suggestions;
      // suggestions'ƒ± kaldƒ±r (ES mapping'de yok)
      delete transformedSuggestionData.suggestions;
    }

    return {
      id: suggestion.id,
      category_id: suggestion.category_id,
      category_name: suggestion.category_name,
      category_path: suggestion.category_path,
      suggestion_type: suggestion.suggestion_type,
      suggestion_data: transformedSuggestionData,
      confidence_score: suggestion.confidence_score,
      is_approved: suggestion.is_approved,
      created_at: suggestion.created_at,
      updated_at: suggestion.updated_at,
      search_boost: suggestion.search_boost || 1.0,
      usage_count: suggestion.usage_count || 0,
      last_used_at: suggestion.last_used_at
    };
  }

  /**
   * Listing'i Elasticsearch formatƒ±na √ßevir
   */
  private transformListingForElasticsearch(listing: any): any {
    return {
      id: listing.id,
      title: listing.title,
      description: listing.description,
      category: listing.category,
      category_id: listing.category_id,
      category_path: listing.category_path,
      budget: listing.budget,
      location: listing.location || null,
      urgency: listing.urgency,
      attributes: listing.attributes || {},
      user_id: listing.user_id,
      status: listing.status,
      created_at: listing.created_at,
      updated_at: listing.updated_at,
      popularity_score: listing.popularity_score || 0,
      is_premium: listing.is_premium || false,
      tags: listing.tags || []
    };
  }

  /**
   * Job status'unu g√ºncelle (Enhanced)
   */
  private async updateJobStatus(jobId: number, status: string, errorMessage?: string): Promise<void> {
    try {
      const updateData: any = {
        status,
        processed_at: status === 'completed' || status === 'failed' ? new Date().toISOString() : null,
        error_message: errorMessage
      };

      // Retry count'u sadece failed durumunda artƒ±r
      if (status === 'failed') {
""        // √ñnce mevcut retry count'u al
        const { data: currentJob } = await this.supabase
          .from('elasticsearch_sync_queue')
          .select('retry_count')
          .eq('id', jobId)
          .single();
        
        if (currentJob) {
          updateData.retry_count = (currentJob.retry_count || 0) + 1;
        }
      }

      const { error } = await this.supabase
        .from('elasticsearch_sync_queue')
        .update(updateData)
        .eq('id', jobId);

      if (error) {
        logger.error('‚ùå Error updating job status:', error);
      }
    } catch (error) {
      logger.error('‚ùå Error updating job status:', error);
    }
  }

  /**
   * Queue istatistiklerini al (Enhanced)
   */
  async getQueueStats(): Promise<QueueStats> {
    try {
      // Direct SQL query instead of RPC
      const { data, error } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('status, created_at, processed_at');

      if (error) {
        logger.error('‚ùå Error getting queue stats:', error);
        return this.stats;
      }

      // Calculate stats manually
      const total = data.length;
      const pending = data.filter((job: any) => job.status === 'pending').length;
      const processing = data.filter((job: any) => job.status === 'processing').length;
      const completed = data.filter((job: any) => job.status === 'completed').length;
      const failed = data.filter((job: any) => job.status === 'failed').length;

      // Calculate average processing time
      const completedJobs = data.filter((job: any) => 
        job.status === 'completed' && job.processed_at && job.created_at
      );
      
      let avgProcessingTime = 0;
      if (completedJobs.length > 0) {
        const totalTime = completedJobs.reduce((sum: number, job: any) => {
          const created = new Date(job.created_at).getTime();
          const processed = new Date(job.processed_at!).getTime();
          return sum + (processed - created);
        }, 0);
        avgProcessingTime = totalTime / completedJobs.length;
      }

      // Get last processed time
      const lastProcessedJob = data
        .filter((job: any) => job.processed_at)
        .sort((a: any, b: any) => new Date(b.processed_at!).getTime() - new Date(a.processed_at!).getTime())[0];

      // Stuck job'larƒ± da say
      const stuckJobs = await this.detectStuckJobs();
      
      this.stats = {
        total,
        pending,
        processing,
        completed,
        failed,
        stuck: stuckJobs.length,
        avgProcessingTime,
        lastProcessedAt: lastProcessedJob?.processed_at
      };

      return this.stats;
    } catch (error) {
      logger.error('‚ùå Error getting queue stats:', error);
      return this.stats;
    }
  }

  /**
   * Stats'ƒ± g√ºncelle
   */
  private async updateStats(): Promise<void> {
    await this.getQueueStats();
  }

  /**
   * Failed job'larƒ± retry et (Enhanced)
   */
  async retryFailedJobs(): Promise<number> {
    try {
      const { data: failedJobs, error } = await this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .eq('status', 'failed')
        .lt('retry_count', this.maxRetries);

      if (error) {
        logger.error('‚ùå Error fetching failed jobs:', error);
        return 0;
      }

      let retryCount = 0;
      for (const job of failedJobs) {
        await this.updateJobStatus(job.id, 'pending', 'Manual retry');
        retryCount++;
      }

      logger.info(`üîÑ Retried ${retryCount} failed jobs`);
      return retryCount;
    } catch (error) {
      logger.error('‚ùå Error retrying failed jobs:', error);
      return 0;
    }
  }

  /**
   * Queue'yu temizle (Enhanced)
   */
  async clearQueue(status?: string): Promise<number> {
    try {
      let query = this.supabase
        .from('elasticsearch_sync_queue')
        .delete();

      if (status) {
        query = query.eq('status', status);
      }

      const { count, error } = await query;

      if (error) {
        logger.error('‚ùå Error clearing queue:', error);
        return 0;
      }

      logger.info(`üóëÔ∏è Cleared ${count} jobs from queue`);
      return count;
    } catch (error) {
      logger.error('‚ùå Error clearing queue:', error);
      return 0;
    }
  }

  /**
   * Queue job'larƒ±nƒ± getir (filtreli)
   */
  async getQueueJobs(status?: string, operation?: string, limit: number = 50, offset: number = 0): Promise<any[]> {
    try {
      let query = this.supabase
        .from('elasticsearch_sync_queue')
        .select('*')
        .order('created_at', { ascending: false })
        .range(offset, offset + limit - 1);

      if (status) {
        query = query.eq('status', status);
      }

      if (operation) {
        query = query.eq('operation', operation);
      }

      const { data: jobs, error } = await query;

      if (error) {
        logger.error('‚ùå Error getting queue jobs:', error);
        return [];
      }

      return jobs || [];
    } catch (error) {
      logger.error('‚ùå Error getting queue jobs:', error);
      return [];
    }
  }

  /**
   * Queue health durumunu kontrol et
   */
  async getHealthStatus(): Promise<{
    isHealthy: boolean;
    issues: string[];
    recommendations: string[];
  }> {
    const issues: string[] = [];
    const recommendations: string[] = [];

    try {
      const stats = await this.getQueueStats();
      
      // Stuck job kontrol√º
      if (stats.stuck > 0) {
        issues.push(`${stats.stuck} stuck jobs detected`);
        recommendations.push('Run health check to reset stuck jobs');
      }

      // Failed job kontrol√º
      if (stats.failed > 10) {
        issues.push(`${stats.failed} failed jobs (high failure rate)`);
        recommendations.push('Review failed jobs and fix underlying issues');
      }

      // Processing job kontrol√º
      if (stats.processing > 20) {
        issues.push(`${stats.processing} jobs stuck in processing`);
        recommendations.push('Check if queue processor is running properly');
      }

      // Pending job kontrol√º
      if (stats.pending > 100) {
        issues.push(`${stats.pending} pending jobs (queue backlog)`);
        recommendations.push('Increase processing frequency or batch size');
      }

      const isHealthy = issues.length === 0;

      return {
        isHealthy,
        issues,
        recommendations
      };

    } catch (error) {
      logger.error('‚ùå Error getting health status:', error);
      return {
        isHealthy: false,
        issues: ['Unable to check queue health'],
        recommendations: ['Check database connection and queue processor']
      };
    }
  }
}

export default QueueProcessorService; 